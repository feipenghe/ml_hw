{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = models.alexnet(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.require_grad = False\n",
    "model.classifier[6] = nn.Linear(4096, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%% args\n"
    }
   },
   "outputs": [],
   "source": [
    "num_of_classes = 10\n",
    "lr = 0.05\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%% Data loader\n"
    }
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "CIFAR10_train = torchvision.datasets.CIFAR10(\".\", train = True, transform = transform_train)\n",
    "CIFAR10_val = torchvision.datasets.CIFAR10(\".\", train = True, transform = transform_train)\n",
    "CIFAR10_test = torchvision.datasets.CIFAR10(\".\", train = False, transform = transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIFAR10_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "val_ratio = 0.1\n",
    "N = len(CIFAR10_train)\n",
    "np.random.seed(10)\n",
    "idx = np.random.randint(0, N, size = N)\n",
    "split = int(N * val_ratio)\n",
    "train_idx, val_idx = idx[split:], idx[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_ratio = 0.1 #\n",
    "train_loader = torch.utils.data.DataLoader(CIFAR10_train, batch_size = batch_size, sampler = train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(CIFAR10_train, batch_size = batch_size, sampler = val_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(CIFAR10_test, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%% Train vs Validation\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.6966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(5.8216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.6389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.8245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(4.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.6256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.4754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(3.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.5459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(2.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.7434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.9657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.8199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.2560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.4277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.6072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(1.5003, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a990cf51d30c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# if using batchnorm or dropout, use train mode setting! don't want to adjust normalization on non-train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# move to device and zero optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "torch.cuda.set_device(0)\n",
    "model = model.cuda(0)\n",
    "# model = torch.nn.DataParallel(model).cuda(0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "log_train_loss = []\n",
    "log_val_loss = []\n",
    "log_val_acc = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\t# training\n",
    "    model.train()  # if using batchnorm or dropout, use train mode setting! don't want to adjust normalization on non-train data\n",
    "    epoch_train_loss = 0\n",
    "    for batch_idx, (input, target) in enumerate(train_loader):\n",
    "        input = input.cuda(0, non_blocking=True) # move to device and zero optimizer\n",
    "        target = target.cuda(0, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        ### train step ###\n",
    "        output = model(input)   # forward\n",
    "        loss = criterion(output, target)\n",
    "        print(loss)\n",
    "        ### end train step ###\n",
    "        ### backward pass and optim step ###\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ### logging\n",
    "        epoch_train_loss += loss\n",
    "    log_train_loss.append(epoch_train_loss/ (batch_idx + 1))\n",
    "\n",
    "\n",
    "    # evaluation\n",
    "    model.eval() # set batchnorm + dropout in eval so it doesn't adjust on validation data\n",
    "    with torch.no_grad(): # turn off gradients\n",
    "        epoch_val_loss = 0\n",
    "        num_correct = 0\n",
    "        highest_val_acc = 0\n",
    "        for batch_idx, (input, target) in enumerate(val_loader):\n",
    "            # do the same steps for train step as for val step but skip updates and backward pass (no gradients)\n",
    "            input = input.cuda(0, non_blocking= True)\n",
    "            target = target.cuda(0, non_blocking= True)\n",
    "            # log val loss every val step\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            epoch_val_loss += loss\n",
    "            # validation accuracy\n",
    "            num_correct_per_batch = torch.sum(target == torch.argmax(output, axis=1))\n",
    "            num_correct += num_correct_per_batch\n",
    "        val_accuracy = num_correct.item() / split\n",
    "        log_val_acc.append(val_accuracy)\n",
    "        print(\"validation accuracy: \", val_accuracy)\n",
    "        if val_accuracy > highest_val_acc:\n",
    "            highest_val_acc = val_accuracy\n",
    "        log_val_loss.append(epoch_val_loss/ (batch_idx + 1)) # average the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "M = len(CIFAR10_test)\n",
    "def eval_test_acc(M, test_loader):\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(test_loader):\n",
    "            input = input.cuda(0, non_blocking= True)\n",
    "            target = target.cuda(0, non_blocking= True)\n",
    "            output = model(input)\n",
    "            num_correct_per_batch = torch.sum(target == torch.argmax(output, axis=1))\n",
    "            num_correct += num_correct_per_batch\n",
    "    print(\"accuracy: \", num_correct.item()/M)\n",
    "\n",
    "eval_test_acc(M, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Fine tuning\n"
    }
   },
   "outputs": [],
   "source": [
    "model2 = models.alexnet(pretrained=True)\n",
    "model2 = model2.cuda(0)\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    epoch_train_loss\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        input = input.cuda(0, non_blocking=True)\n",
    "        target = target.cuda(0, non_blocking= True)\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss\n",
    "    log_epoch_train\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def report(name, loss1, loss2, val_acc_l, test_acc):\n",
    "    plt.plot(loss1, label = \"Train Loss\")\n",
    "    plt.plot(loss2, label = \"Validation loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "    print(\"Highest accuracy: \", max(val_acc_l))\n",
    "    print(\"Final test accuracy: \", test_acc)\n",
    "    plt.savefig(name)\n",
    "\n",
    "\n",
    "report(\"A4a Transfer Learning\", log_train_loss, log_val_loss, log_val_acc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(torch.version)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
